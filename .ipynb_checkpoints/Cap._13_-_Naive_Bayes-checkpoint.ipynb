{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Naive Bayes\n",
    "\n",
    "Uma rede social não é tão boa se as pessoas não conseguem se conectar. Portanto, a DataSciencester possui um atributo popular que permite que membros enviem mensagens uns aos outros. E enquanto a maioria dos membros são cidadãos responsáveis que somente enviam mensagens de “como você está?”, alguns são canalhas e enviam mensagens de spam sobre esquemas para ficarem ricos, medicamentos sem receita e programas de credenciamento de data science. Seus usuários começaram a reclamar e a Vice-presidente de Mensagem pediu que você usasse data science para descobrir como filtrar essas mensagens de spam.\n",
    "\n",
    "### Um Filtro de Spam Muito Estúpido\n",
    "\n",
    "* Imagine um \"universo\" que consiste em receber uma mensagem escolhida ao acaso entre todas as possíveis.\n",
    "  * S: a mensagem é spam.\n",
    "  * V: A mensagem contém a palavra \"viagra\".\n",
    "  \n",
    "  \n",
    "* O teorema de Bayes (Cap. 06) diz que a probabilidade de uma mensagem ser spam depender da palavra \"viagra\" é dado por:\n",
    "\n",
    "   $P(S|V) = [P(V|S)*P(S)] / [P(V|S)*P(S)+P(V|not(S))*P(not(S))]$\n",
    "\n",
    "* Esse cálculo pode ser visto como a proporção de mensagens com a palavra \"viagra\" que são spam.\n",
    "* Se temos uma grande quantidade de mensagens que sabemos que não são spam e outra grande qauntidade que sabemos ser spam, então é possível calcular $P(V|S)$ e $P(V|not(S))$.\n",
    "* Se presumimos que uma mensagem possui igual probabilidade de ser spam ou não spam ($P(S) = P(not(S)) = 0.5$), então:\n",
    "\n",
    "   $P(S|V) = P(V|S)/[P(V|S)+P(V|not(S))]$\n",
    "\n",
    "* Ex.: 50% das mensagens de spam possuem a palavra \"viagra\". 1% das mensagens não spam possuem. A probabilidade de que qualquer email que tenha a palavra \"viagra\" seja spam é:\n",
    "\n",
    "   $0.5/(0.5+0.01) = 98\\%$\n",
    "\n",
    "### Um Filtro de Spam Mais Sofisticado\n",
    "\n",
    "* Imagine que temos um vocabulário de muitas palavras $w_{1}$, ..., $w_{n}$.\n",
    "* Usaremos $X_{i}$ para o evento \"a mensagem contém a palavra $w_{i}$\".\n",
    "* Imagine também que encontramos um $P(X_{i}|S)$ como estimativa da probabilidade de a mensagem de spam conter a palavra $w_{i}$ e $P(X_{i}|not(S))$ de a mensagem de não spam conter a palavra $w_{i}$.\n",
    "\n",
    "* A chave para naive bayes é fazer a suposição de que as presenças (ou ausências) de cada palavra são independentes uma das outras, condição para uma mensagem ser spam ou não.\n",
    "  * Significa que a presença de uma palavra na mensagem não indica a presença de outra. Matematicamente:\n",
    "  \n",
    "    $P(X_{1} = x_{1}, ..., X_{n} = x_{n}|S) = P(X_{1} = x_{1}|S) * ... * P(X_{n} = x_{n}|S)$\n",
    "    \n",
    "* É uma hipótese extrema.\n",
    "* Imagine que todo o nosso vocabulário consista apenas das palavras “viagra” e “rolex”, e que metade de todas as mensagens de spam sejam “viagra barato” e que a outra metade seja “rolex autêntico”.\n",
    "* A estimativa que um spam conhtenha os dois é:\n",
    "\n",
    "   $P(X_{1} = 1, X_{2} = 1|S) = P(X_{1} = 1|S)*P(X_{2} = 1|S) = 0.5*0.5 = 0.25$\n",
    "   \n",
    "* assim, afastamos a teoria de que \"viagra\" e \"rolex\" não apareçam juntos.\n",
    "* O Teorema de Bayes também diz que podemos calcular a probabilidade de uma mensagem ser spam usando a equação:\n",
    "\n",
    "   $P(S|X = x) = P(X = x|S)/[P(X = x|S)+P(X = x|not(S))]$\n",
    "   \n",
    "* A suposição de Naive Bayes permite calcular cada uma das probabilidades multiplicando junto as estimativas de probabilidade de cada uma das palavras.\n",
    "* Na prática a múltiplicação de muitas probabilidades ao mesmo tempo é evitada para evitar underflow.\n",
    "* Esse cálculo então é feito de forma mais amigável ao computador usando a seguinte fórmula:\n",
    "\n",
    "   $exp(log(p_{1})+...+log(p_{n}))$\n",
    "   \n",
    "* Não devemos calcular a probabilidade de $P(X_{i}|S)$ como uma fração de mensagens contendo a palavra $w_{i}$.\n",
    "* Isso, quando  pode fazer com que a probabilidade da mensagem ser spam seja 0, mesmo que a mensagem tenha muitas outras palavras que a indiquem como um spam.\n",
    "* Para esse calculo utilizamos um valor k para que a probabilidade da palavra tenha um valor mínimo que não prejudique o resultado final. A fórmula final fica assim:\n",
    "\n",
    "   $P(X_{i}|S)$ = (k + número de spams contendo $w_{i}$ / (2k + número de spams)\n",
    "   \n",
    "* O mesmo deve ser feito para $P(X_{i}|S)$.\n",
    "* Ao adicionar k presumimos que mesmo que não apareça a palavra em nenhuma mensagem de spam, é possível que ela apareça em alguma mensagem de spam desconhecida.\n",
    "* Ex.: se \"dado\" ocorre em 0/98 documentos spam e se k = 1, $P(\"dado\"|S)$ é calculado como:\n",
    "\n",
    "   $1/100 = 0.001$\n",
    "   \n",
    "* Isso permite que o classificador atribua alguma probabilidade diferente de 0 para mensagens que contenham a palavra \"dado\".\n",
    "\n",
    "### Implementação\n",
    "\n",
    "* Iniciamos criando uma função bem simples para quebrar mensagens em palavras distintas.\n",
    "* A segunda função contará as palavras em um conjunto de palavras rotuladas para treino.\n",
    "  * O retorno será um dicionário no qual as palavras são as chaves e os valores são listas com 2 valores indicando a quantidade de vezes que a palavra foi vista em mensagens spam e em mensagens não spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Função para quebrar mensagens\n",
    "def tokenize(message):\n",
    "    message = message.lower() # Converte tudo para minúsculo\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message) # Extrai as palavras\n",
    "    return set(all_words) # Remove duplicatas\n",
    "\n",
    "# Função para contar palavras\n",
    "def count_words(training_set):\n",
    "    counts = defaultdict(lambda: [0,0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1]+=1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* O passo seguinte é transformar as contagens em probabilidades.\n",
    "  * O retorno será uma lista de triplas contendo (palavra, prob spam, prob não spam)\n",
    "  \n",
    "\n",
    "* A última parte será usar as probabilidades das palavras para atribuir probabilidades a mensagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Calcula a probabilidade das palavras\n",
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    return [(w,\n",
    "            (spam+k)/(total_spams+2*k),\n",
    "            (non_spam+k)/(total_non_spams+2*k))\n",
    "           for w, (spam, non_spam) in counts.items()]\n",
    "\n",
    "# Usa as probabilidades das palavras para atribuir probabilidades as mensagens\n",
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "    \n",
    "    # Itera sobre cada palavra em nosso vocabulário\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "        \n",
    "        # se \"word\" aparecer na mensagem,\n",
    "        # adicione a probabilidade log de vê-la\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam+=math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam+=math.log(prob_if_not_spam)\n",
    "        \n",
    "        # se \"word\" não aparecer na mensagem\n",
    "        # adicione a probabilidade log de não vê-la\n",
    "        # que é log(1-probabilidade de vê-la)\n",
    "        else:\n",
    "            log_prob_if_spam+=math.log(1.0-prob_if_spam)\n",
    "            log_prob_if_not_spam+=math.log(1.0-prob_if_not_spam)\n",
    "        \n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam+prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* E então podemos juntar tudo isso no classificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        \n",
    "        # Conta mensagens de spam e não spam\n",
    "        num_spams = len([is_spam\n",
    "                        for message, is_spam in training_set\n",
    "                        if is_spam])\n",
    "        num_non_spams = len(training_set)-num_spams\n",
    "        \n",
    "        # roda dados de treinamento pela nossa \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,\n",
    "                                            num_spams,\n",
    "                                            num_non_spams,\n",
    "                                            self.k)\n",
    "        \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o Nosso Modelo\n",
    "\n",
    "* Para o teste do nosso modelo, utilizaremos o conjunto de dados SpamAssassin public corpus, apesar de ser um conjunto antigo.\n",
    "* Para o teste utilizaremos apenas o assunto de cada email.\n",
    "* Todas as linhas referentes ao assunto começam com \"Subject:\"\n",
    "  * Procuraremos por isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares\n",
    "# Cap. 11\n",
    "random.seed(0)\n",
    "def split_data(x, prob):\n",
    "    results = [],[]\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results\n",
    "\n",
    "def accuracy(tp, fp, fn, tn):\n",
    "    correct = tp+tn\n",
    "    total = tp+fp+fn+tn\n",
    "    return correct/total\n",
    "\n",
    "# Sensibilidade\n",
    "def recall(tp, fp, fn, tn):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "path = 'Dados/spam/*/*'\n",
    "\n",
    "data = []\n",
    "\n",
    "for fn in glob.glob(path):\n",
    "    is_spam = \"ham\" not in fn\n",
    "    \n",
    "    with open(fn, 'r', encoding=\"utf8\", errors='ignore') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                # remove o primeiro \"Subject:\" e mantém o que sobrou\n",
    "                subject = re.sub(r\"^Subject: \", \"\", line).strip()\n",
    "                data.append((subject, is_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "# Iremos dividir o nosso conjunto em Treino e Teste\n",
    "\n",
    "train_data, test_data = split_data(data, 0.75)\n",
    "\n",
    "# Realiza o Treino\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_data)\n",
    "\n",
    "# Verificando como o nosso modelo faz\n",
    "classified = [(subject, is_spam, classifier.classify(subject))\n",
    "              for subject, is_spam in test_data]\n",
    "\n",
    "# presuma que spam_probability > 0.5 corresponde à previsão de spam\n",
    "# e conta as combinações de (is_spam real, is_spam previsto)\n",
    "counts = Counter((is_spam, spam_probability > 0.5) \n",
    "                 for _, is_spam, spam_probability in classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 90.18%\n",
      "Sensibilidade: 65.67%\n"
     ]
    }
   ],
   "source": [
    "# Obtendo os resultados e calculando as métricas\n",
    "tp = counts.get((True, True))\n",
    "fp = counts.get((False, True))\n",
    "fn = counts.get((True, False))\n",
    "tn = counts.get((False, False))\n",
    "print(\"Acurácia: {:.2f}%\".format(accuracy(tp, fp, fn, tn)*100))\n",
    "print(\"Sensibilidade: {:.2f}%\".format(recall(tp, fp, fn, tn)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos dar uma olhada nos mais mal classificados\n",
    "classified.sort(key=lambda row: row[2])\n",
    "\n",
    "# as maiores probabilidades de spam previstos entre os não-spams\n",
    "spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]\n",
    "\n",
    "# as menores probabilidades de spam previstos entre os spams\n",
    "hammiest_spams = list(filter(lambda row: row[1], classified))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The MIME information you requested (last changed 3154 Feb 14)', False, 0.9977012929188156), ('Save up to 70% on international calls!', False, 0.998256216404264), ('Four free e-mailers reviewed, Get the gear you need to burn DVDs', False, 0.9993376854790142), ('=?iso-8859-1?Q?Matrox_Parhelia=99_now_available?=', False, 0.999692131524301), ('\"I meditated in a cave for 12 years and now I\\'m here to tell you', False, 0.9999830867163033)]\n"
     ]
    }
   ],
   "source": [
    "print(spammiest_hams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I was so scared... my very first DP', True, 6.082339995944272e-05), ('Re: girls', True, 0.0008412798514905131), ('.Message report from your contact page....//ytu855 rkq', True, 0.0021595543693716636), ('Looking for property in SPAIN?', True, 0.002842605425644866), ('[scoop] ....It is not my fault. .- vwiid', True, 0.0046969498498913775)]\n"
     ]
    }
   ],
   "source": [
    "print(hammiest_spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver quais as palavras que possuem mais jeito de spam\n",
    "def p_spam_given_word(word_prob):\n",
    "    \"\"\"usa o teorema de bayes para computar p(spam | message contains word)\"\"\"\n",
    "    # word_prob é uma das triplas produzidas por word_probabilities\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "\n",
    "spammiest_words = words[-5:]\n",
    "hammiest_words = words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sale', 0.031081081081081083, 0.0002294630564479119),\n",
       " ('money', 0.03648648648648649, 0.0002294630564479119),\n",
       " ('systemworks', 0.03648648648648649, 0.0002294630564479119),\n",
       " ('adv', 0.03918918918918919, 0.0002294630564479119),\n",
       " ('rates', 0.041891891891891894, 0.0002294630564479119)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spammiest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spambayes', 0.0013513513513513514, 0.04749885268471776),\n",
       " ('users', 0.0013513513513513514, 0.040614960991280404),\n",
       " ('was', 0.0013513513513513514, 0.03832033042680129),\n",
       " ('razor', 0.0013513513513513514, 0.0346489215236347),\n",
       " ('sadev', 0.0013513513513513514, 0.03051858650757228)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammiest_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Como poderíamos obter melhores resultados?\n",
    "  * A maneira óbvia seria obter mais dados para treino.\n",
    "  \n",
    "\n",
    "* Existem várias maneiras de melhorar o modelo.\n",
    "* Algumas possibilidades são:\n",
    "  * Olhe o conteúdo da mensagem todo.\n",
    "  * O classificador implementado leva em consideração cada palavra que aparece no conjunto de treinamento, até mesmo as palavras que só aparecem uma vez. Modifique o classificador para aceitar um limite opcional min_count e ignore os símbolos que não aparecem tantas vezes.\n",
    "  * O tokenizer não tem percepção de palavras similares (por exemplo, “cheap” e “cheapest”). Modifique o classificador para ter uma função stemmer que converte palavras para as classes equivalentes de palavras.\n",
    "    * Criar uma boa função stemmer é difícil. Porter Stemmer é geralmente a mais usada.\n",
    "  * Mesmo que todas as nossas características sejam “mensagens contendo a palavra $w_{i}$ ́ ”, não há motivo para tal. Em nossa implementação, nós pudemos acrescentar características extras como “mensagem contendo um número” criando tokens fictícios como contains:number e modificando o tokenizer para emiti-los quando necessário."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
