{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Gradiente Descendente\n",
    "\n",
    "### A ideia por trás do Gradiente Descendente\n",
    "\n",
    "* Suponha que tenhamos a função f que tem como entrada um vetor de números reais e exibe, como saída, um único número real. \n",
    "* Tal função simples é:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    return sum(v_i**2 for v_i in v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Com frequência será necessário maximizar ou minimizar tais funções.\n",
    "* Para funções como essa, o gradiente mostra a direção da entrada em que a função cresce mais rapidamente.\n",
    "* Uma abordagem para maximizar uma função é:\n",
    "  * Pegar um ponto de início aleatório;\n",
    "  * Computar o gradiente;\n",
    "  * Andar um pequeno passo na direção do gradiente e;\n",
    "  * Repetir com o novo ponto inicial.\n",
    "  \n",
    "  \n",
    "* Para minimizar, são realizados os mesmos passos na direção oposta.\n",
    "\n",
    "![Maximizando grad. descendente](Imgs/Minimization_image.png)\n",
    "\n",
    "### Estimando o gradiente\n",
    "\n",
    "* Se f é uma função de uma variável, sua derivada em um ponto x indica como f(x) muda quando fazemos uma mudança bem pequena em x.\n",
    "* Definida como limite de quocientes diferenciais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_quotient(f,x,h):\n",
    "    return (f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aproximando derivada](Imgs/apx_deriv.png)\n",
    "\n",
    "* A derivada é a inclinação da linha tangente em (x, f(x)).\n",
    "* O quociente diferencial é a inclinação da linha-não-tão-tangente que passa por (x+h, f(x+h)).\n",
    "* Conforme h diminui, a linha-não-tão-tangente vai ficando mais próxima da linha tangente.\n",
    "* Para muitas funções é fácil calcular as derivadas com exatidão. \n",
    "* Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "# E a derivada\n",
    "def derivative(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Podemos estimar derivadas ao avaliar o quociente diferencial por um pequeno $e$. Veja o gráfico abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(-10,10)\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "plt.plot(x, list(map(derivative, x)), 'rx', label='Actual')\n",
    "plt.plot(x, list(map(derivative_estimate, x)), 'b+', label='Estimate') # azul +\n",
    "plt.legend(loc=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Quando f é uma função de muitas variáveis, possui múltiplas derivadas parciais.\n",
    "* Cada derivada indica como f muda com pequenas mudanças em uma das variáveis.\n",
    "* Calcula-se a derivada parcial do i-ésimo ao tratar como uma função de apenas a i-ésima variável e depois podemos estimar o gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "    # Computa o i-ésimo quociente diferencial parcial de f em v\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "        for j, v_j in enumerate(v)]\n",
    "    \n",
    "    return (f(w)-f(v))/h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f,v,i,h)\n",
    "           for i,_ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o Gradiente\n",
    "\n",
    "* Usaremos os gradientes para encontrar o mínimo entre todos os vetores tridimensionais.\n",
    "* Pegaremos um ponto inicial aleatório e daremos pequenos passos na direção oposta do gradiente. \n",
    "* Será repetido até que cheguemos em um ponto em que o gradiente seja muito pequeno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares cap. 04 (Álgebra Linear)\n",
    "from Codigos.linear_algebra import vector_add, vector_subtract, vector_sum, vector_mean, dot, sum_of_squares, magnitude, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordenadas do ponto mínimo:  [-3.7009791374886747e-06, -2.3131119609304243e-06, 2.3131119609304243e-06]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def step(v, direction, step_size):\n",
    "    # Move step_size na direção a partir de v\n",
    "    return [v_i + step_size * direction_i\n",
    "           for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "v = [random.randint(-10,10) for i in range(3)] # Coords Ponto inicial aleatório\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v) # Computa o gradiente no ponto v\n",
    "    next_v = step(v, gradient, -0.01) # calcula um novo ponto\n",
    "    if distance(next_v, v) < tolerance: # para se estiver convergindo\n",
    "        break\n",
    "    v = next_v # Continua se não estiver\n",
    "    \n",
    "print(\"Coordenadas do ponto mínimo: \", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolhendo o tamanho do próximo passo\n",
    "\n",
    "* Existem 3 opções populares para escolher o tamanho do próximo passo do gradiente, são elas:\n",
    "  1. Usar um passo de tamanho fixo\n",
    "  2. Diminuir gradualmente o tamanho do passo a cada vez\n",
    "  3. A cada passo, escolher o tamanho do passo que minimize o valor da função objetiva.\n",
    "  \n",
    "  \n",
    "* A última opção, apesar de parecer perfeita, é uma computação custosa.\n",
    "* Pode-se aproximá-la ao tentar uma variedade de passos que resulte no menor valor da função objetiva: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* É possível que alguns tamanhos resultem em entradas inválidas.\n",
    "* Para tal, é necessário criar uma função \"segura\" que retorna infinito para entradas inválidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntando tudo\n",
    "\n",
    "* No geral, temos alguma target_fn que queremos minimizar e também temos o seu gradient_fn.\n",
    "* Digamos que escolhemos um valor inicial para os parâmetros theta_0.\n",
    "* Podemos implementar o gradiente descendente como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    theta = theta_0\n",
    "    target_fn = safe(target_fn)\n",
    "    value = target_fn(theta)\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient,-step_size) \n",
    "                       for step_size in step_sizes]\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        if abs(value-next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chamamos de minimize_batch porque, para cada passo do gradiente, ele considera o conjunto inteiro de dados.\n",
    "* Podemos maximizar uma função minimizando o seu negativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                         negate_all(gradient_fn),\n",
    "                         theta_0,\n",
    "                         tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente Descendente Estocástico\n",
    "\n",
    "* Nas abordagens anteriores, cada passo gradiente requer façamos uma previsão e computemos o gradiente para o conjunto de dados inteiro.\n",
    "* Essa fuunções de erro normalmente são aditivas, ou seja, o erro previsto é a soma dos erros previstos para cada ponto.\n",
    "* O gradiente descendente estocástico computa o gradiente apenas um ponto de cada vez.\n",
    "* Circula os dados repetidamente até alcançar um ponto de parada.\n",
    "* Durante os ciclos, os dados são iterados de forma aleatória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    indexes = [i for i, _ in enumerate(data)]\n",
    "    random.shuffle(indexes)\n",
    "    for i in indexes:\n",
    "        yield data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Andaremos um passo gradiente para cada ponto de dados.\n",
    "* O método permite que circulemos o ponto mínimo até que paremos de obter melhorias. \n",
    "* A partir de então, diminuiremos o passo até o ponto de parada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    data = zip(x, y)\n",
    "    theta = theta_0 # palpite inicial\n",
    "    alpha = alpha_0 # tamanho do passo inicial\n",
    "    min_theta, min_value = None, float(\"inf\") # o mínimo até agora\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # se formos até 100 iterações sem melhorias, paramos\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "    \n",
    "    if value < min_value:\n",
    "        # se achou um novo mínimo, lembre-se\n",
    "        # e volte para o tamanho do passo original\n",
    "        min_theta, min_value = theta, value\n",
    "        iterations_with_no_improvement = 0\n",
    "        alpha = alpha_0\n",
    "    else:\n",
    "        # do contrário, não estamos melhorando, portanto tente\n",
    "        # diminuir o tamanho do passo\n",
    "        iterations_with_no_improvement += 1\n",
    "        alpha *= 0.9\n",
    "        \n",
    "        # e ande um passo gradiente para todos os pontos de dados\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            \n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A versão estocástica será tipicamente mais rápida que a versão batch.\n",
    "* Uma função que maximiza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn), \n",
    "                               negate_all(gradient_fn), \n",
    "                               x, y, theta_0, alpha_0=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
